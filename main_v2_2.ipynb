{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_v2_2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9TK7RM-WzBSj","colab_type":"text"},"source":["Dane to zbiór CelebA. 30000 zdjęć twarzy celebrytów wyśrodkowanych na oczy oraz przyciętych do rozmiaru 256x256.\n","Dane przechowywane są na Google Drive,"]},{"cell_type":"code","metadata":{"id":"riRaNMgTT0vE","colab_type":"code","outputId":"6ce17689-982d-4e02-925a-10e7366a460a","executionInfo":{"status":"ok","timestamp":1570069180467,"user_tz":-120,"elapsed":2402,"user":{"displayName":"Maciej Nawrocki","photoUrl":"","userId":"01048005128497125520"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G-johg-Azwat","colab_type":"text"},"source":["Zdjęcia przetworzone zostają z przestrzeni barw RGB na LAB. LAB pozwala na latwe rozdzielenie zdjęć na warswę z jasnością oraz 2 warstwy odpowiadające za kolor zdjęcia. \n","\n","Zakres wartości warstwy L to od 0 do 100, AB od -128 do 128. Rozdzielone warstwy są od razu normalizowane.\n","\n","By do modelu trafily dane tych samych rozmiarów rozdzielone warstwy zostają uzupelnione 0."]},{"cell_type":"code","metadata":{"id":"MWp-lpO7UScV","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","def load_img_to_lab(img_path):\n","    img = io.imread(img_path)\n","    img = np.array(img, dtype=float)\n","    img = color.rgb2lab(1/255.0*img)\n","    return img\n","\n","def split_to_l_ab(img):\n","    l = img [:,:,0]\n","    ab = img [:,:,1:]\n","    l=l/100\n","    ab=ab/128\n","    return l, ab\n","\n","def combine_l_ab(l, ab):\n","    shape = (256, 256, 3)\n","    img = np.zeros(shape)\n","    l=l*100\n","    ab = ab*128\n","    img[:, :, 0] = l[:, :, 0] \n","    img[:, :, 1] = ab[:, :, 1]\n","    img[:, :, 2] = ab[:, :, 2]\n","    return img\n","  \n","def display_from_lab(img):\n","    img=color.lab2rgb(img)\n","    plt.set_cmap('gray')\n","    plt.imshow(img)\n","    \n","def display_from_l(l):\n","    shape = (256, 256, 3)\n","    img = np.zeros(shape)\n","    l=l*100\n","    img[:, :, 0] = l[:, :, 0] \n","    img = color.lab2rgb(img)\n","    plt.imshow(img)\n","    \n","def make_img_from_l (l):\n","    shape = (256, 256, 3)\n","    img = np.zeros(shape)\n","    img[:, :, 0] = l[:, :]\n","    return img\n","    \n","def make_img_from_ab (ab):\n","    shape = (256, 256, 3)\n","    img = np.zeros(shape)\n","    img[:, :, 1] = ab[:, :, 0]\n","    img[:, :, 2] = ab[:, :, 1]\n","    return img    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QUgo4W5kCC8X","colab_type":"text"},"source":["Baza danych zostala podzielona na 10 części po 3000 zdjęć. Taki rozmiar umożliwia trenowanie modelu na platformie Colab."]},{"cell_type":"code","metadata":{"id":"NqTEngX4BLmx","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","def load_data_from_path(path, skip=0, limit=0):\n","    x_data=[]\n","    y_data=[]\n","    \n","    if skip != 0: \n","      left=0\n","    \n","    if limit != 0: \n","      stop=0\n","    for filename in os.listdir(path):\n","        if skip != 0 and left < skip:\n","          left  = left + 1  \n","        else:\n","          l, ab = split_to_l_ab(load_img_to_lab(path+filename))\n","          x_data.append(make_img_from_l(l))\n","          y_data.append(make_img_from_ab(ab))\n","          if (stop + 1) % 500 == 0:\n","                print(str(stop+1)+' images in!')\n","        if limit != 0:\n","          stop = stop+1\n","          if stop == limit:\n","            break\n","    x_data=np.array(x_data, dtype=float)\n","    y_data=np.array(y_data, dtype=float)\n","    return x_data, y_data  \n","\n","def make_dataset_from_arrays(x_data, y_data, batch=1):\n","    x_data = np.expand_dims(x_data, axis=1)\n","    y_data = np.expand_dims(y_data, axis=1)\n","    x_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_data, tf.float32), tf.cast(y_data, tf.float32)))\n","    x_dataset.batch(batch)\n","    return x_dataset\n","  \n","def make_dataset_ready(data_path, part, part_size=3000):\n","    x_data, y_data=[],[]\n","    down_bound = 0 + part_size*part\n","    up_bound = part_size+part_size*part\n","    mid_bound1 = down_bound + part_size/3\n","    mid_bound2 = mid_bound1 + part_size/3\n","    \n","    x_data, y_data = load_data_from_path(data_path,skip=down_bound, limit=mid_bound1)\n","    x_dataset = make_dataset_from_arrays(x_data, y_data)\n","    \n","    x_data, y_data = load_data_from_path(data_path,skip=mid_bound1, limit=mid_bound2)\n","    x_dataset2 = make_dataset_from_arrays(x_data, y_data)\n","    x_dataset = x_dataset.concatenate(x_dataset2)\n","    del x_dataset2\n","    \n","    x_data, y_data = load_data_from_path(data_path,skip=mid_bound2, limit=up_bound)\n","    x_dataset2 = make_dataset_from_arrays(x_data, y_data)\n","    x_dataset = x_dataset.concatenate(x_dataset2)\n","    #del those variables to prevent OOM on next iteration\n","    del x_data\n","    del y_data\n","    del x_dataset2\n","    print('Dataset made! size: ' + str(part_size) + ' part: ' + str(part) + ' photos from ' + str(down_bound) + ' to ' + str(up_bound))\n","    return x_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQm8LoGbmOSX","colab_type":"text"},"source":["Użyty model bazuje na modelu pix2pix z pracy \"Image-to-Image Translation with Conditional Adversarial Networks\" (https://arxiv.org/abs/1611.07004)\n","\n","Model zgodnie z zalożeniami GAN sklada się z generatora i dyskryminatora:\n","\n","Generator bazuje na modelu U-net:\n","*   Zbudowany z 2 części - kodera i dekodera\n","*   Koder zbudowany z warstw: Conv2D -> Leaky ReLU -> Batch Normalization\n","*   Dekoder zbudowany z warstw: UpSampling -> Conv2D -> Batch Normalization\n","*   Między każdą warstwą kodera i dekodera są przejścia \n","\n","Dyskryminator na modelu PatchGAN:\n","*   Zbudowany z warstw: Conv -> BatchNorm -> Leaky ReLU\n","*   Wyjście zwracane z ostatniej warswty ma rozmiar 30x30\n","*   Każdy fragment 30x30 odpowiada fragmentowi 70x70 danych wejściowych\n","*   Do dyskryminatora podawane są dwa obrazy wejściowe:\n","   *   Obraz z rzeczywistych barw zdjęcia który powinien zostać zakwalifikowany jako prawdziwy\n","   *   Obraz z generatora który zostanie oceniony   \n","   *   Oba obrazy są lączone\n","   \n","Trenowanie:\n","*   Na wejście generatora podawany jest obraz w skali szarości, model generuje obraz wyjściowy\n","*   Na wejście dyskryminatora podawany jest obraz wygenerowany oraz rzeczywisty obraz barw \n","*   Następnie przeliczane są wartości funkcji blędu generatora i dyskryminatora\n","*   Następnie wartości blędu podawane są do optymalizatora\n"]},{"cell_type":"markdown","metadata":{"id":"WC0B9qtUIUi4","colab_type":"text"},"source":["Funkcje generatora i dyskryminatora charakterystyczne dla wersji:"]},{"cell_type":"code","metadata":{"id":"PeTTSz0rIQYc","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","def build_generator():\n","  \n","        def conv2d(layer_input, filters, f_size=4, bn=True):\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same',use_bias=False)(layer_input)\n","            if bn:\n","                d = BatchNormalization(momentum=0.8)(d)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            return d\n","\n","        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","            u = Conv2DTranspose(filters, kernel_size=f_size, strides=2,padding='same',use_bias=False)(layer_input)\n","            u = BatchNormalization(momentum=0.8)(u)\n","            if dropout_rate:\n","                u = Dropout(dropout_rate)(u)\n","            u = ReLU()(u)\n","            u = Concatenate()([u, skip_input])\n","            return u\n","        \n","        # Image input\n","        d0 = Input(shape=INPUT_IMG_SHAPE) #(bs, 256, 256, 1)\n","        \n","        # Downsampling\n","        d1 = conv2d(d0, GEN_F, bn=False) #(bs, 128, 128, 64)\n","        d2 = conv2d(d1, GEN_F*2) #(bs, 64, 64, 128)\n","        d3 = conv2d(d2, GEN_F*4) #(bs, 32, 32, 256)\n","        d4 = conv2d(d3, GEN_F*8) #(bs, 16, 16, 512)\n","        d5 = conv2d(d4, GEN_F*8) #(bs, 8, 8, 512)\n","        d6 = conv2d(d5, GEN_F*8) #(bs, 4, 4, 512)\n","        d7 = conv2d(d6, GEN_F*8) #(bs, 2, 2, 512)\n","        d8 = conv2d(d7, GEN_F*8) #(bs, 1, 1, 512)\n","\n","        # Upsampling\n","        u0 = deconv2d(d8, d7, GEN_F*8, dropout_rate=0.5) #(bs, 2, 2, 512) \n","        u1 = deconv2d(u0, d6, GEN_F*8, dropout_rate=0.5) #(bs, 4, 4, 512) \n","        u2 = deconv2d(u1, d5, GEN_F*8, dropout_rate=0.5) #(bs, 8, 8, 512)\n","        u3 = deconv2d(u2, d4, GEN_F*8) #(bs, 16, 16, 512)\n","        u4 = deconv2d(u3, d3, GEN_F*4) #(bs, 32, 32, 256)\n","        u5 = deconv2d(u4, d2, GEN_F*2) #(bs, 64, 64, 128)\n","        u6 = deconv2d(u5, d1, GEN_F) #(bs, 128, 128, 64)\n","\n","        u7 = UpSampling2D(size=2)(u6) # (bs, 256, 256, 3)\n","        output_img = Conv2D(OUTPUT_CHANNELS, kernel_size=4, strides=1, padding='same', activation='tanh')(u7) # (bs, 256, 256, 3)\n","        \n","        return Model(d0, output_img)\n","\n","def build_discriminator():\n","  \n","    def d_layer(layer_input, filters, f_size=4, bn=True):\n","        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","        if bn:\n","            d = BatchNormalization(momentum=0.8)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","        return d\n","\n","\n","    img_inp = Input(shape=INPUT_IMG_SHAPE, name='input_image')\n","    img_tar = Input(shape=OUTPUT_IMG_SHAPE, name='target_image')\n","\n","\n","    # Concatenate image and conditioning image by channels to produce input\n","    combined_imgs = Concatenate(axis=-1)([img_inp, img_tar]) #(bs, 256, 256, channels*2)\n","\n","    d1 = d_layer(combined_imgs, DISC_F, bn=False) # (bs, 128, 128, 64)\n","    d2 = d_layer(d1, DISC_F*2) #(bs, 64, 64, 128)\n","    d3 = d_layer(d2, DISC_F*4) #(bs, 32, 32, 256)\n","    d4 = d_layer(d3, DISC_F*8) #(bs, 16, 16, 512)\n","  \n","    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n","\n","    return Model([img_inp, img_tar], validity)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWXnVNCRIzWa","colab_type":"text"},"source":["Funkcje blędu i trenowanie niecharakterystyczne dla wersji:"]},{"cell_type":"code","metadata":{"id":"ZMkDJm5vgsON","colab_type":"code","cellView":"both","colab":{}},"source":["#@title\n","def discriminator_loss(disc_real_output, disc_generated_output):\n","    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n","    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","    total_disc_loss = real_loss + generated_loss\n","    return total_disc_loss\n","\n","def generator_loss(disc_generated_output, gen_output, target):\n","    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","    # mean absolute error\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","    return total_gen_loss\n","  \n","\n","def train_step(input_image, target, first=0):\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      gen_output = generator(input_image, training=True)\n","\n","      disc_real_output = discriminator([input_image, target], training=True)\n","      disc_generated_output = discriminator([input_image, gen_output], training=True)\n","\n","      gen_loss = generator_loss(disc_generated_output, gen_output, target)\n","      disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n","    if first:\n","      return gen_loss, disc_loss\n","    else:\n","      return 0, 0\n","  \n","def fit(train_ds, epochs, path, version_str, samples=1 ,save_checkpoint=0):  \n","    epoch_first_gen_loss=0\n","    epoch_first_disc_loss=0\n","    if save_checkpoint:\n","      checkpoints_path = path + 'checkpoints/'\n","      if not os.path.isdir(checkpoints_path):\n","        os.mkdir(checkpoints_path)\n","\n","    for epoch in range(epochs):\n","      start = time.time()\n","      first = 1 \n","      for input_image, target in train_ds:\n","        if first:\n","          epoch_first_gen_loss, epoch_first_disc_loss = train_step(input_image, target, first)\n","          first=0\n","        else:\n","          train_step(input_image, target)\n","\n","      # generating samples every epoch\n","      checkpoint_str = version_str + '-epoch-' + str(epoch)\n","      if samples:\n","        output1, output2 = make_2samples(generator)\n","        out_sample1.append(output1)\n","        out_sample2.append(output2)\n","        samples_names.append(checkpoint_str)\n","\n","      # saving (checkpoint) the model every x epochs  \n","      if (save_checkpoint != 0) and (epoch % save_checkpoint == 0) and (epoch !=0):\n","        save_model(generator, checkpoints_path, checkpoint_str, model_type='gen')\n","        save_model(discriminator, checkpoints_path, checkpoint_str, model_type='discr')\n","      \n","      loss_vec_epoch.append(epoch)\n","      loss_vec_gen.append(epoch_first_gen_loss)\n","      loss_vec_disc.append(epoch_first_disc_loss)\n","      print (\"[Epoch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, epochs,\n","                                                                          epoch_first_gen_loss,\n","                                                                          epoch_first_disc_loss,\n","                                                                          time.time()-start))\n","\n","    \n","def save_model(model, path, version, model_type=''):\n","    #no need for JSON, model structure is known\n","    #serialize weights to HDF5\n","    if model_type == 'gen': model_name = 'modelGEN-' + version\n","    elif model_type == 'discr': model_name = 'modelDISCR-' + version\n","    else: model_name = 'modelX-' + version\n","    model.save_weights(path + model_name + '.m5')\n","    print('Saved model to disk: ' + model_name)\n","\n","def load_models(path, version, activation=0):\n","    if activation:\n","      gen_load_name = 'modelGEN-' + version + '.m5'\n","      disc_load_name = 'modelDISCR-' + version + '.m5'\n","      generator.load_weights(path + gen_load_name)\n","      discriminator.load_weights(path + disc_load_name)\n","\n","def make_2samples(model):\n","    x_input = in_2samples\n","    inp = tf.cast(x_input[0], tf.float32)\n","    gen_output = generator(inp[tf.newaxis,...], training=False)\n","    gen_output = tf.squeeze(gen_output,0)\n","    output1 = combine_l_ab(inp, gen_output) \n","    \n","    inp = tf.cast(x_input[1], tf.float32)\n","    gen_output = generator(inp[tf.newaxis,...], training=False)\n","    gen_output = tf.squeeze(gen_output,0)\n","    output2 = combine_l_ab(inp, gen_output)\n","    \n","    return output1, output2\n","\n","def show_2samples(path, samples1, samples2, samples_names, save=0, show=1):\n","    x_input = in_2samples\n","    sample1 = x_input[0]\n","    sample2 = x_input[1]\n","    if save:\n","      if not os.path.isdir(path):\n","        os.mkdir(path)\n","    for c, value in enumerate(samples1, 0):\n","      fig = plt.figure(figsize=(10,10))\n","      \n","      fig.add_subplot(1, 2, 1)\n","      fig.text(-0.25, 0.5, samples_names[c],\n","      horizontalalignment='center',\n","      verticalalignment='center',\n","      fontsize=15)\n","      display_from_lab(samples1[c])\n","      plt.axis('off')\n","      \n","      fig.add_subplot(1, 2, 2)\n","      display_from_lab(samples2[c])    \n","      plt.axis('off')\n","      \n","      fig.tight_layout()\n","      \n","      if save:\n","        plt.savefig(path + samples_names[c] + '.png', bbox_inches='tight')\n","      if show:\n","        plt.show()\n","    if save: \n","      print(\"Saved samples to disk\")\n","    \n","def test_10samples(model, path, model_name='', save=0, show=1):\n","    x_data=[]\n","    y_data=[]\n","    x_data, y_data = load_data_from_path(path+'input/',limit=11)\n","    if save:\n","      dir_path = path + model_name + '/'\n","      if not os.path.isdir(dir_path):\n","        os.mkdir(dir_path)\n","    \n","    for c, value in enumerate(x_data, 0):\n","      sample = x_data[c]\n","      real_ab = y_data[c]\n","      real = combine_l_ab(sample,real_ab)\n","      \n","      inp = tf.cast(sample, tf.float32)\n","      gen_output = generator(inp[tf.newaxis,...], training=False)\n","      gen_output = tf.squeeze(gen_output,0)\n","      \n","      title = ['Real Image','Grey Image', 'Colorized Image']\n","      plt.figure(figsize=(15,15))\n","\n","      for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","        if i==0 : \n","          display_from_lab(real)\n","        elif i==1 :\n","          display_from_l(sample)\n","        elif i==2 : \n","          display_from_lab(combine_l_ab(inp, gen_output))    \n","        plt.axis('off')\n","      if save:\n","        plt.savefig(dir_path + str(c+1) + '.png', bbox_inches='tight')\n","      if show:\n","        plt.show()\n","    \n","    if save: \n","      print(\"Saved samples to disk\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-rBVwLOJeFx","colab_type":"text"},"source":["Biblioteki:"]},{"cell_type":"code","metadata":{"id":"4FcCUqvKgnOA","colab_type":"code","cellView":"form","outputId":"44ec5e38-2b1a-4375-bba0-149899debedc","executionInfo":{"status":"ok","timestamp":1570069182103,"user_tz":-120,"elapsed":3560,"user":{"displayName":"Maciej Nawrocki","photoUrl":"","userId":"01048005128497125520"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#@title\n","\n","import tensorflow as tf\n","\n","import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from skimage import io, color\n","import os\n","\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from tensorflow.keras.layers import LeakyReLU, ReLU\n","from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import load_model\n","import time\n","import sys\n","\n","tf.enable_eager_execution()\n","\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.Session(config=config)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"hwLMQoXUJ13t","colab_type":"text"},"source":["Zmienne globalne i zbudowanie modeli:"]},{"cell_type":"code","metadata":{"id":"2Y4SEoajgpnB","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","\n","IMG_ROWS = 256\n","IMG_COLS = 256\n","INPUT_CHANNELS = 3\n","OUTPUT_CHANNELS = 3\n","INPUT_IMG_SHAPE = (IMG_ROWS, IMG_COLS, INPUT_CHANNELS)\n","OUTPUT_IMG_SHAPE = (IMG_ROWS, IMG_COLS, OUTPUT_CHANNELS)\n","\n","# Number of filters in the first layer of G and D\n","GEN_F = 64\n","DISC_F = 64\n","LAMBDA = 100\n","loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","generator_optimizer = Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = Adam(2e-4, beta_1=0.5)\n","\n","generator = build_generator()\n","discriminator = build_discriminator()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXJ92T4KKK2j","colab_type":"text"},"source":["Ścieżki do istotnych folderów i 2 zdjęcia testowe: "]},{"cell_type":"code","metadata":{"id":"nfLjNBtVJ-tb","colab_type":"code","cellView":"both","colab":{}},"source":["#@title\n","model_path = '/content/drive/My Drive/Models&Checkpoints/'\n","data_path='/content/drive/My Drive/Celeba-data256x256/'\n","samples10_path='/content/drive/My Drive/10samples/'\n","if not os.path.isdir(model_path): \n","  os.mkdir(model_path)\n","  \n","#2 test images from 2samples:\n","sample1_path='/content/drive/My Drive/2samples/input/4.jpg'\n","sample2_path='/content/drive/My Drive/2samples/input/7.jpg'\n","l_sample1, ab_sample1 = split_to_l_ab(load_img_to_lab(sample1_path))\n","l_sample2, ab_sample2 = split_to_l_ab(load_img_to_lab(sample2_path))\n","in_2samples = [make_img_from_l(l_sample1), make_img_from_l(l_sample2)]\n","out_sample1 = []\n","out_sample2 = []\n","samples_names = []\n","\n","loss_vec_epoch = []\n","loss_vec_gen = []\n","loss_vec_disc = []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3u_yr1sCkgRR","colab_type":"text"},"source":["Komórka do ćwiczenia modelu:"]},{"cell_type":"code","metadata":{"id":"TmDgYGRC0dhq","colab_type":"code","outputId":"3265045e-2993-4a13-b8c1-f9ccdca66e8b","executionInfo":{"status":"ok","timestamp":1570092713180,"user_tz":-120,"elapsed":19763633,"user":{"displayName":"Maciej Nawrocki","photoUrl":"","userId":"01048005128497125520"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fluSRq51v5OeKAeQqszi9keHsIwuxfR_"}},"source":["EPOCHS = 20\n","PART = 4\n","model_version = '2.3'\n","\n","\"\"\"old_part_str = '0' + str(PART-1)\n","old_version_str = 'Version' + model_version + '-epoch' + str(EPOCHS) + '-datapart' + old_part_str \n","#before launching check if previous version had same name formula and correct model path\n","load_models(model_path, old_version_str, PART)\"\"\"\n","generator.load_weights(model_path+ 'model_gen_v2-epoch20-datapart_03.m5')\n","discriminator.load_weights(model_path+ 'model_discr_v2-epoch20-datapart_03.m5')\n","\n","part_str = '0' + str(PART)\n","version_str = 'Version' + model_version + '-epoch' + str(EPOCHS) + '-datapart' + part_str \n","path_2samples = '/content/drive/My Drive/2samples/'+ version_str + '/'\n","\n","x_dataset = make_dataset_ready(data_path, PART, part_size=3000)\n","fit(x_dataset, EPOCHS, model_path, version_str, samples=1, save_checkpoint=5)\n","\n","save_model(generator, model_path, version_str, model_type='gen')\n","save_model(discriminator, model_path, version_str, model_type='discr')\n","\n","show_2samples(path_2samples, out_sample1, out_sample2, samples_names, save=1, show=1)\n","\n","gen_model_name = 'modelGEN-' + version_str\n","test_10samples(generator, samples10_path, gen_model_name, save=1, show=1)\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"roOwgK01_Ee-","colab_type":"code","colab":{}},"source":["array_size = len(loss_vec_epoch)\n","loss_array_shape = (array_size,3)\n","loss_array=np.zeros(loss_array_shape)\n","loss_array[:,0] = loss_vec_epoch\n","loss_array[:,1] = loss_vec_gen\n","loss_array[:,2] = loss_vec_disc\n","np.save(model_path + 'losses_'+ version_str + '.npy',loss_array)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J3-K9IwikoAj","colab_type":"text"},"source":["Komórka do zaladowania i stestowania modelu (nieużywana w trenowaniu modelu):"]},{"cell_type":"code","metadata":{"id":"BLGLrbzytzki","colab_type":"code","cellView":"both","outputId":"b0b45580-c11c-49e1-aee3-07e1e4876674","executionInfo":{"status":"error","timestamp":1570092714449,"user_tz":-120,"elapsed":1304,"user":{"displayName":"Maciej Nawrocki","photoUrl":"","userId":"01048005128497125520"}},"colab":{"base_uri":"https://localhost:8080/","height":418}},"source":["#@title\n","version_str = 'FinalVersion'\n","\n","checkpoint_str = 'modelGEN-FinalVersion.m5'\n","generator.load_weights(model_path + checkpoint_str )\n","gen_model_name = 'modelGEN-' + version_str\n","#test_10samples(generator, samples10_path, gen_model_name, save=1, show=1)\n","\n","myphoto_path = '/content/drive/My Drive/MyPhoto/'\n","test_10samples(generator, myphoto_path,gen_model_name, save=1, show=1)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c40742f66cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'modelGEN-FinalVersion.m5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcheckpoint_str\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgen_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'modelGEN-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mversion_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#test_10samples(generator, samples10_path, gen_model_name, save=1, show=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    161\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/drive/My Drive/Models&Checkpoints/modelGEN-FinalVersion.m5"]}]}]}